{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import nbformat as nbf\n",
    "import shutil\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\n",
    "conversion_map = {\n",
    "    'textfield': 'text',\n",
    "    'text': 'text',\n",
    "    'integer': ('numerical', 'Int64'),\n",
    "    'decimal': ('numerical', 'Float'),\n",
    "    'date': {'sdtype': 'datetime', 'datetime_format': '%Y-%m-%d'},\n",
    "    'single': 'categorical',\n",
    "    'radio': 'categorical',\n",
    "    'text_multiline': 'text',\n",
    "    'list': 'text',\n",
    "    'checkbox': 'categorical',\n",
    "    'textmultiline': 'text',\n",
    "    'multiples': 'categorical',\n",
    "    'multiple': 'categorical'\n",
    "}\n",
    "\n",
    "\n",
    "def import_csv_folder(folder_path):\n",
    "    \"\"\"Looks for the right folders to go fetch all the informations\n",
    "\n",
    "    Args:\n",
    "        folder_path (path): Folder path\n",
    "\n",
    "    Returns:\n",
    "        metadata and dicos: Returns the metadata and dico list\n",
    "    \"\"\"\n",
    "    if folder_path:\n",
    "        structure_folder_path = os.path.join(folder_path, '1_structure')\n",
    "        link_folder_path = os.path.join(folder_path, '2_link/link.csv')\n",
    "        dico_folder_path = os.path.join(folder_path, '4_dico/dico.csv')\n",
    "        if os.path.exists(structure_folder_path) and \\\n",
    "        os.path.isdir(structure_folder_path):\n",
    "            all_files = glob.glob(os.path.join(structure_folder_path, '*.csv'))\n",
    "            fichier = pd.read_csv(link_folder_path, sep=';')\n",
    "            dicos = pd.read_csv(dico_folder_path, sep = ';')\n",
    "            return convert_to_metadata(all_files, fichier, dicos)\n",
    "        else:\n",
    "            print(\"'1_structure' folder not found in the selected folder.\")\n",
    "\n",
    "\n",
    "def import_json_file(json_file_path, calc):\n",
    "    \"\"\"Reads the JSON file\n",
    "\n",
    "    Args:\n",
    "        json_file_path (path): The path to JSON file\n",
    "\n",
    "    Returns:\n",
    "        json and dico list: Returns the metadata and dico list\n",
    "    \"\"\"\n",
    "    if json_file_path:\n",
    "        json_code = read_json_file(json_file_path)\n",
    "        return convert_json_to_metadata(json_code, calc)\n",
    "\n",
    "\n",
    "def read_json_file(json_file_path):\n",
    "    \"\"\"Reads the JSON File\n",
    "\n",
    "    Args:\n",
    "        json_file_path (path): Path to JSON file\n",
    "\n",
    "    Returns:\n",
    "        json_file: The read JSON file\n",
    "    \"\"\"\n",
    "    json_file_path_js = json_file_path + \".json\"\n",
    "    with open(json_file_path_js, 'r', encoding='utf-8') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "\n",
    "def convert_to_metadata(all_files, link, dico):\n",
    "    \"\"\"Converts all the CSVs to SDV Metadata\n",
    "\n",
    "    Args:\n",
    "        all_files (list): A list of all tables\n",
    "        link (list): A list of relationships\n",
    "        dico (dict): A dictionnary\n",
    "\n",
    "    Returns:\n",
    "        metadata and list: Returns the right SDV Metadata format with the \n",
    "        dictionnaries\n",
    "    \"\"\"\n",
    "    metadata_json = {\n",
    "        \"METADATA_SPEC_VERSION\": \"MULTI_TABLE_V1\",\n",
    "        \"tables\": {},\n",
    "        \"relationships\": [],\n",
    "    }\n",
    "    \n",
    "    listo = []\n",
    "\n",
    "    for filename in all_files:\n",
    "\n",
    "        df = pd.read_csv(filename, sep=\";\")\n",
    "        table_name = df.loc[df[\"type\"] == \"P\", \"varset\"].to_string(index=False)\n",
    "        specs = df.loc[df[\"type\"] == \"V\"]\n",
    "        table_specs = {\n",
    "            \"primary_key\": \"\",\n",
    "            \"columns\": {},\n",
    "            \"column_relationships\": [],\n",
    "        }\n",
    "\n",
    "        for line in specs.iterrows():\n",
    "            big_dico = {'table_name':\"\",\n",
    "            'col': \"\",\n",
    "            'type': \"\",\n",
    "            'values':[]}\n",
    "            column_id = line[1][\"field_name\"]\n",
    "            sdtype = line[1][\"field_type\"]\n",
    "\n",
    "            if sdtype == 'radio':\n",
    "                big_dico['table_name'] = table_name\n",
    "                \n",
    "                big_dico['col'] = column_id\n",
    "                type = line[1]['dico']\n",
    "                big_dico['type'] = type\n",
    "                values = dico[dico['dico_name'] == type]\n",
    "                for value in values.iterrows():\n",
    "                    big_dico['values'].append(value[1]['code'])\n",
    "                listo.append(big_dico)\n",
    "\n",
    "            sdtype = conversion_map.get(sdtype, sdtype)\n",
    "            if isinstance(sdtype, tuple):\n",
    "                table_specs[\"columns\"][column_id] \\\n",
    "                = {\"sdtype\": sdtype[0], \"computer_representation\": sdtype[1]}\n",
    "            elif isinstance(sdtype, dict): \n",
    "                table_specs[\"columns\"][column_id] = sdtype\n",
    "            else:\n",
    "                table_specs[\"columns\"][column_id] = {\"sdtype\": sdtype}\n",
    "\n",
    "        metadata_json[\"tables\"][table_name] = table_specs\n",
    "    for index, all in link.iterrows():\n",
    "        parent = all['varset_1']\n",
    "        child = all['varset_2']\n",
    "        nom_id = parent + '.id_data'\n",
    "        relation = {\n",
    "            'parent_table_name': parent,\n",
    "            'child_table_name': child,\n",
    "            'parent_primary_key': 'id_data',\n",
    "            'child_foreign_key': nom_id\n",
    "        }\n",
    "        metadata_json['tables'][parent]['columns']['id_data'] \\\n",
    "        = {\"sdtype\": \"id\"}\n",
    "        metadata_json['tables'][child]['columns']['id_data'] = {\"sdtype\": \"id\"}\n",
    "        metadata_json['tables'][child]['columns'][nom_id] = {\"sdtype\": \"id\"}\n",
    "        metadata_json['tables'][parent]['primary_key'] = 'id_data'\n",
    "        metadata_json['tables'][child]['primary_key'] = 'id_data'\n",
    "        metadata_json['tables'][child]['foreign_key'] = nom_id\n",
    "        metadata_json['relationships'].append(relation)\n",
    "\n",
    "    return metadata_json, listo\n",
    "\n",
    "\n",
    "def convert_json_to_metadata(root, calc):\n",
    "    \"\"\"Takes in the JSON file and converts it to the metadata using imbeded \n",
    "    functions\n",
    "\n",
    "    Args:\n",
    "        root (json_dict): The raw JSON imported file\n",
    "        calc (bool): The setting to take account the calculted values\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def convert_to_metadata_json(liste, liens, dicos, rep):\n",
    "        \"\"\"Convert the JSON file to the right SDV Metadata format\n",
    "\n",
    "        Args:\n",
    "            liste (list): Tables descriptions\n",
    "            liens (list): Tables relationships\n",
    "            dicos (dict): A list that contains all the datasets dictionnary\n",
    "            rep (bool): The setting to take account the calculted values\n",
    "\n",
    "        Returns:\n",
    "            json and list: Returns a metadata and dataset dictionnary for the\n",
    "            dictionnary conversion\n",
    "        \"\"\"\n",
    "        metadata_json = {\n",
    "            \"METADATA_SPEC_VERSION\": \"MULTI_TABLE_V1\",\n",
    "            \"tables\":{},\n",
    "            \"relationships\": []\n",
    "        }\n",
    "\n",
    "        listo = []\n",
    "        linked_tables = []\n",
    "\n",
    "        for m in range(len(liens)):\n",
    "            parent = liens[m][0]['name']\n",
    "            child = liens[m][1]['name']\n",
    "            linked_tables.append(parent)\n",
    "            linked_tables.append(child)\n",
    "\n",
    "        for i in range(len(liste)):\n",
    "            if liste[i]['nom'] in list(set(linked_tables)) or len(liste) == 1:\n",
    "                table_names = liste[i]['nom']\n",
    "                tables_specs =  {\n",
    "                    \"primary_key\": \"\",\n",
    "                    \"columns\": {},\n",
    "                    \"column_relationships\": []\n",
    "                }\n",
    "                for j in range(len(liste[i]['valeur'])):\n",
    "                    big_dico = {'table_name':\"\",\n",
    "                        'col': \"\",\n",
    "                        'type': \"\",\n",
    "                        'values':[]}\n",
    "                    colonne = liste[i]['valeur'][j]['nom']\n",
    "                    sdtype = liste[i]['valeur'][j]['type']\n",
    "                    if sdtype == 'radio' or sdtype == 'single':\n",
    "                        dico_id = liste[i]['valeur'][j]['dico']\n",
    "                        for k in range(len(dicos)):\n",
    "                            if dicos[k]['id'] == dico_id:\n",
    "                                big_dico['table_name'] = table_names\n",
    "                                big_dico['col'] = colonne\n",
    "                                big_dico['type'] = sdtype\n",
    "                                for l in range(len(dicos[k]\\\n",
    "                                                   ['attrs']['value'])):\n",
    "                                    big_dico['values']\\\n",
    "                                        .append(dicos[k]['attrs']['value']\\\n",
    "                                                [l]['code'])\n",
    "                        listo.append(big_dico)\n",
    "\n",
    "\n",
    "                    \n",
    "                    sdtype = conversion_map.get(sdtype, sdtype)\n",
    "                    if isinstance(sdtype, tuple):\n",
    "                        tables_specs[\"columns\"][colonne] \\\n",
    "                            = {\"sdtype\": sdtype[0], \n",
    "                               \"computer_representation\": sdtype[1]}\n",
    "                    elif isinstance(sdtype, dict): \n",
    "                        tables_specs[\"columns\"][colonne] = sdtype\n",
    "                    elif sdtype == 'calculated' and rep == True:\n",
    "                        tables_specs[\"columns\"][colonne] = {\"sdtype\": 'text'}\n",
    "                    elif sdtype == 'calculated' and rep == False:\n",
    "                        continue\n",
    "                    else:\n",
    "                        tables_specs[\"columns\"][colonne] = {\"sdtype\": sdtype}\n",
    "\n",
    "                tables_specs[\"columns\"]['sys_id'] = {\"sdtype\" : \"id\" }\n",
    "                tables_specs['primary_key'] = 'sys_id'\n",
    "                metadata_json['tables'][table_names] = tables_specs\n",
    "            else:\n",
    "                print(f\"The table {liste[i]['nom']} is not explicitly linked.\")\n",
    "                continue\n",
    "\n",
    "        for s in range(len(liens)):\n",
    "            relat = {\n",
    "                \"parent_table_name\": \"\",\n",
    "                \"child_table_name\": \"\",\n",
    "                \"parent_primary_key\": \"\",\n",
    "                \"child_foreign_key\": \"\"\n",
    "            }\n",
    "            parent = liens[s][0]['name']\n",
    "            child = liens[s][1]['name']\n",
    "\n",
    "            relat['parent_table_name'] = parent\n",
    "            relat['child_table_name'] = child\n",
    "            relat['parent_primary_key'] \\\n",
    "                = metadata_json['tables'][parent]['primary_key']\n",
    "            parent_key = parent + '.sys_id'\n",
    "            metadata_json['tables'][child]['columns'][parent_key] \\\n",
    "                = {'sdtype' : 'id'}\n",
    "            relat['child_foreign_key'] = parent_key\n",
    "\n",
    "            metadata_json['relationships'].append(relat)\n",
    "\n",
    "        return metadata_json, listo\n",
    "\n",
    "\n",
    "    def extract_component_info(data, nam):\n",
    "        \"\"\"Recursive function to find all variables within the pages of a \n",
    "        Voozanoo 4 project\n",
    "\n",
    "        Args:\n",
    "            data (json_dict): the JSON file converted to understandable python\n",
    "            nam (string): The name of the page/table/varset\n",
    "\n",
    "        Returns:\n",
    "            metadate (dict) : Specific format dictionnary that will allow \n",
    "            the convert function to work\n",
    "        \"\"\"\n",
    "        metadate = {'nom': nam, 'valeur': []}\n",
    "        if isinstance(data, dict):\n",
    "            attrs = data.get('attrs', {})\n",
    "            try:\n",
    "                name = attrs['name']\n",
    "                render_type = attrs.get('render-type')\n",
    "                subtype = attrs.get('subtype')\n",
    "                label_position = attrs.get('labelPosition')\n",
    "                \n",
    "                if attrs.get('type') == 'component' and render_type != 'form':\n",
    "                    if render_type == 'single' or render_type == 'multiples' \\\n",
    "                        or render_type == 'multiple':\n",
    "                        dico = attrs.get('dico')\n",
    "                        metadate['valeur'].append({'nom': name, \n",
    "                                                   'type': render_type, \n",
    "                                                   'dico': dico})\n",
    "                    elif subtype == 'boolean' and label_position:\n",
    "                        metadate['valeur'].append({'nom': name, \n",
    "                                                   'type': 'boolean'})\n",
    "                    else:\n",
    "                        metadate['valeur'].append({'nom': name, \n",
    "                                                   'type': render_type})\n",
    "                elif attrs.get('type') == 'datasource' and \\\n",
    "                    attrs.get('subtype') == 'custom':\n",
    "                    label = attrs.get('label', '').lower()\n",
    "                    label = unicodedata.normalize('NFKD', \n",
    "                                                  label)\\\n",
    "                                                    .encode('ascii', 'ignore')\\\n",
    "                                                        .decode('utf-8')\n",
    "                    label = re.sub(r'[^a-zA-Z0-9_]', '_', label)\n",
    "                    metadate['valeur'].append({'nom': label, \n",
    "                                               'type': 'calculated'})\n",
    "            except KeyError:\n",
    "                pass\n",
    "            for child in data.get('child', []):\n",
    "                child_metadate = extract_component_info(child, nam)\n",
    "                metadate['valeur'].extend(child_metadate['valeur'])\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                child_metadate = extract_component_info(item, nam)\n",
    "                metadate['valeur'].extend(child_metadate['valeur'])\n",
    "        return metadate\n",
    "\n",
    "\n",
    "    def parse_pages(data):\n",
    "        \"\"\"Parse pages using a recursive function to look for the right ones\n",
    "        and use the extract_component function to extract all the variables to\n",
    "        create the right format for the final converter\n",
    "\n",
    "        Args:\n",
    "            data (json_dict): JSON portion that contains all the pages \n",
    "            of the project\n",
    "\n",
    "        Returns:\n",
    "            metadates (list): A list of all the tables\n",
    "            that the project contains under the right input format for the \n",
    "            final converter\n",
    "        \"\"\"\n",
    "        metadates = []\n",
    "        if isinstance(data, dict):\n",
    "            attrs = data.get('attrs', {})\n",
    "            component_type = attrs.get('type', '')\n",
    "            component_subtype = attrs.get('subtype', '')\n",
    "            if component_type == 'component' and component_subtype == 'page':\n",
    "                if attrs.get('render-type', '') == 'form':\n",
    "                    no = data['attrs']['varset']\n",
    "                    metadate = extract_component_info(data, no)\n",
    "                    metadates.append(metadate)\n",
    "            for child in data.get('child', []):\n",
    "                metadates.extend(parse_pages(child))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                metadates.extend(parse_pages(item))\n",
    "        return metadates\n",
    "\n",
    "\n",
    "    def parse_dicos(data):\n",
    "        \"\"\"Look for the dictionnaries informations and creates a list of these \n",
    "        informations for the converter to search into\n",
    "\n",
    "        Args:\n",
    "            data (json_dict): JSON portion that contains all the dictionnaries \n",
    "            of the project\n",
    "\n",
    "        Returns:\n",
    "            dicos (list): A list of all the dictionnaries \n",
    "            that the project contains under the right input format for the \n",
    "            final converter\n",
    "        \"\"\"\n",
    "        dicos = []\n",
    "        for item in data:\n",
    "            dicos.append(item)\n",
    "        return dicos\n",
    "\n",
    "\n",
    "    def parse_relationship(data):\n",
    "        \"\"\"Look for the relationships informations and creates a list of these \n",
    "        informations for the converter to search into\n",
    "\n",
    "        Args:\n",
    "            data (json_dict): JSON portion that contains all the relationships \n",
    "            of the project\n",
    "\n",
    "        Returns:\n",
    "            liaisons (list): A list of all the relationships that the project \n",
    "            contains under the right input format for the final converter\n",
    "        \"\"\"\n",
    "        liaisons = []\n",
    "        for item in data:\n",
    "            liaisons.append(item['attrs']['varsets'])\n",
    "        return liaisons\n",
    "\n",
    "\n",
    "    def treat_form_pages(data):\n",
    "        \"\"\"A JSON parser that will parse the JSON file and seperate it a uses \n",
    "        the right funciton to ease the process\n",
    "\n",
    "        Args:\n",
    "            data (json_dict): The read JSON file using the read function.\n",
    "\n",
    "        Returns:\n",
    "            metadates, dicos, liaisons (dicts and lists): Dictionnaries and \n",
    "            Lists to the right format so the convert_to_metadata_json function \n",
    "            can use them to create the right metadata format\n",
    "        \"\"\"\n",
    "        metadates = []\n",
    "        dicos = []\n",
    "        liaisons = []\n",
    "        if isinstance(data, dict):\n",
    "            id = data.get('id')\n",
    "            if id == 'pages':\n",
    "                metadates = parse_pages(data)\n",
    "            elif id == 'dicos':\n",
    "                dicos = parse_dicos(data['child'])\n",
    "            elif id == 'relations':\n",
    "                liaisons = parse_relationship(data['child'])\n",
    "            else:\n",
    "                for child in data.get('child', []):\n",
    "                    result = treat_form_pages(child)\n",
    "                    metadates.extend(result[0])\n",
    "                    dicos.extend(result[1])\n",
    "                    liaisons.extend(result[2])\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                result = treat_form_pages(item)\n",
    "                metadates.extend(result[0])\n",
    "                dicos.extend(result[1])\n",
    "                liaisons.extend(result[2])\n",
    "        return metadates, dicos, liaisons\n",
    "\n",
    "\n",
    "    def merge_dicts_with_same_table_name(metadates):\n",
    "        \"\"\"If a same varset contains multiple pages, the function will merge\n",
    "        them together in order to not create multiple same name tables\n",
    "\n",
    "        Args:\n",
    "            metadates (list): A list of the parsed pages of the the JSON file\n",
    "\n",
    "        Returns:\n",
    "            merged_tables (list): A list with the tables that had the same \n",
    "            names merged with their values merged (same name values will be\n",
    "            merged too)\n",
    "        \"\"\"\n",
    "        merged_metadates = {}\n",
    "        for metadate in metadates:\n",
    "            table_name = metadate['nom']\n",
    "            if table_name not in merged_metadates:\n",
    "                merged_metadates[table_name] = metadate['valeur']\n",
    "            else:\n",
    "                merged_metadates[table_name].extend(metadate['valeur'])\n",
    "        return [{'nom': table_name, 'valeur': values} for table_name, \n",
    "                values in merged_metadates.items()]\n",
    "\n",
    "\n",
    "    metadates, dicos, liaisons = treat_form_pages(root) \n",
    "\n",
    "    merged_metadates = merge_dicts_with_same_table_name(metadates)\n",
    "\n",
    "    metadata, lista = convert_to_metadata_json(merged_metadates, \n",
    "                                               liaisons, \n",
    "                                               dicos, \n",
    "                                               calc)\n",
    "\n",
    "    return metadata, lista\n",
    "\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    \"\"\"Creates the new folder that will contain the new files.\n",
    "\n",
    "    Args:\n",
    "        folder_name (string): The name of the new folder.\n",
    "    \"\"\"\n",
    "    f_n = './data/' + folder_name\n",
    "    os.makedirs(f_n, exist_ok=True)\n",
    "\n",
    "    source = './.mod.py'\n",
    "    shutil.copy(source, f_n)\n",
    "\n",
    "def create_notebook(folder_name, output, met):\n",
    "    \"\"\"Creates the first notebook to first visualize and process the metadata.\n",
    "    It will also make the user modify the metadata so the right sdtypes are \n",
    "    applied.\n",
    "    \n",
    "    Args:\n",
    "        folder_name (str): The name of the folder where to put it into\n",
    "        output (str): The name of the created file\n",
    "        met (str): The name of the SDV JSON file created to load\n",
    "    \"\"\"    \n",
    "    f_n = './data/' + folder_name\n",
    "    os.chdir(f_n)\n",
    "\n",
    "    output_file = output + \".json\"\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(met, f, indent=4)\n",
    "\n",
    "    nb = nbf.v4.new_notebook()\n",
    "\n",
    "\n",
    "    text = \"\"\"# Premiere visualisation\n",
    "Vous avez la possibilite de visualiser le metadata en choisissant dabord le \\\n",
    "bon kernel\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text))\n",
    "\n",
    "    code = f\"\"\"from sdv.metadata import MultiTableMetadata\n",
    "    \\nimport pandas as pd\n",
    "    \\nmetadata = MultiTableMetadata.load_from_json('./{output}.json')\n",
    "    \\nmetadata.visualize()\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code))\n",
    "\n",
    "    text1 = \"\"\"# Generation des donnees sans modifications au Metadata\n",
    "Apres la conversion, le Metadata est pret a l'emploi cependant il est \\\n",
    "possible de le personnaliser.\n",
    "Importons rapidement un synthetiseur pour generer des donnees. \\\n",
    "Vous apprendrez par la suite comment utiliser ce synthetiseur\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text1))\n",
    "\n",
    "    code1 = f\"\"\"from sdv_enterprise.sdv.multi_table.dayz.day_zero import \\\n",
    "        DayZSynthesizer\n",
    "    \\nsynthesizer = DayZSynthesizer(metadata)\n",
    "    \\nsynthetic_data = synthesizer.sample()\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code1))\n",
    "\n",
    "    text2 = \"\"\"Allons voir dans une table les donnees generees.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text2))\n",
    "\n",
    "    code_1 = \"\"\"synthetic_data['nom_dune_table'].head() # N'oubliez pas de \\\n",
    "    changer le nom par un nom d'une de vos tables\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code_1))\n",
    "\n",
    "\n",
    "    text3 = \"\"\"# Vous avez la possibilite de modifier les types\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text3))\n",
    "\n",
    "    phtm_code = \"\"\"# Par exemple\n",
    "# metadata.update_column(\n",
    "#   table_name = 'patient',\n",
    "#   column_name = 'nom',\n",
    "#   sdtype = 'last_name')\n",
    "#\n",
    "# Dans le cas ou vous devez remplacer plusieurs colonnes dans un meme tableau \\\n",
    "vous avez la possibilite d'utiliser cette commande\n",
    "#\n",
    "# metadata.update_columns_metadata(\n",
    "#    table_name='users',\n",
    "#    column_metadata={\n",
    "#        'age': { 'sdtype': 'numerical' },\n",
    "#        'ssn': { 'sdtype': 'ssn', 'pii': True },\n",
    "#        'gender': { 'sdtype': 'categorical' },\n",
    "#        'dob': { 'sdtype': 'datetime', 'datetime_format': '%Y-%m-%d' },\n",
    "#        ...\n",
    "#    }\n",
    "#)\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(phtm_code))\n",
    "\n",
    "    text3_5 = \"\"\"Pour modifier correctement le sdtype suivez \\\n",
    "[ce lien](https://epiconcept.gitbook.io/synthetic-data-vault-sdv/multi-tables/preparation-des-donnees/api-multi-table-metadata#update-api)\n",
    "\n",
    "    \n",
    "Pour plus d'informations sur les sdtypes voir \\\n",
    "[ce lien](https://epiconcept.gitbook.io/synthetic-data-vault-sdv/reference/metadata-spec/sdtypes)\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text3_5))\n",
    "\n",
    "    text4 = \"\"\"# Visualisation et Export\n",
    "Maintenant nous allons pouvoir exporter le nouveau metadata pour la \\\n",
    "generation de donnees. N'oubliez pas de donner un nom au nouveau fichier.\"\"\"   \n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text4))\n",
    "\n",
    "    code2_0 = \"\"\"metadata.visualize()\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code2_0))\n",
    "\n",
    "    code2 = f\"\"\"metadata.save_to_json('./{output}_v2.json')\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code2))\n",
    "\n",
    "    text4 = \"\"\"# Generation des donnees rapide\n",
    "Apres la petite personnalisation, generons un echantillon a partir du \\\n",
    "nouveau metadata.\n",
    "Reutilisons le meme synthetiseur.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text4)) \n",
    "\n",
    "    code3 = f\"\"\"synthesizer = DayZSynthesizer(metadata)\n",
    "    \\nsynthetic_data = synthesizer.sample()\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code3))\n",
    "\n",
    "    code4 = \"\"\"synthetic_data['nom_dune_table'].head() # N'oubliez pas \\\n",
    "de changer le nom par un nom d'une de vos tables\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code4))\n",
    "\n",
    "    text5 = \"\"\"Pour generer des meilleures donnees, nous vous invitons \\\n",
    "de continuer vers le Notebook numero 1.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text5)) \n",
    "\n",
    "    notebook_filename = \"0-Visualisation_et_Changements.ipynb\"\n",
    "    with open(notebook_filename, 'w') as f:\n",
    "        nbf.write(nb, f)\n",
    "    \n",
    "    print(f\"Metadata converted, Notebook created, your folder {folder_name} \\\n",
    "is ready.\")\n",
    "\n",
    "\n",
    "def create_from_scratch(output, dico_list, sep):\n",
    "    \"\"\"Creates the second notebook if the scratch setting was set to TRUE\n",
    "    in order to apply the parsed dictionnaries and sets the seperator of the \n",
    "    exported CSV. If the separator was precised then it will be applied \n",
    "    otherwise the default separator is \",\"\n",
    "    \n",
    "    Args:\n",
    "        output (str): The name of the created file and we will add _v2 in order\n",
    "        to follow the first notebook\n",
    "        dico_list (list): The list of the treated dictionnaries to load into \n",
    "        the synthesizer\n",
    "        sep (str): The default or custom separator for the exported CSV\n",
    "    \"\"\"\n",
    "    os.rename('.mod.py', 'mod.py')\n",
    "    nb = nbf.v4.new_notebook()\n",
    "    \n",
    "    text = \"\"\"# Generation de donnees synthetique a partir du Metadata seul\n",
    "Si vous lancez les cases suivantes, il vous sera possible de generer des \\\n",
    "donnees synthetiques seulement a l'aide de votre metadata. Vous n'avez pas \\\n",
    "besoin d'importer les dictionnaires, cela est fait automatiquement pour vous. \\\n",
    "Il vous suffit juste de verifier si vous avez charge le bon metadata et de \\\n",
    "lancer toutes les cases.\n",
    "## Visualisation\n",
    "Verifiez si votre Metadata est correct\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text))\n",
    "\n",
    "    code = f\"\"\"from sdv.metadata import MultiTableMetadata\n",
    "    \\nimport pandas as pd\n",
    "    \\nmetadata = MultiTableMetadata.load_from_json('./{output}_v2.json')\n",
    "    \\nmetadata.visualize()\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code))\n",
    "\n",
    "\n",
    "    text1 = \"\"\"## Generation des donnees avec les dictionnaires\n",
    "Apres l'import du nouveau Metadata. Importons rapidement un synthetiseur \\\n",
    "pour generer des donnees. Nous avons recupere les dictionnaires de votre \\\n",
    "metadata et les avons implementes dans ce notebook. Il vous suffit de \\\n",
    "tout lancer pour ajouter ces dictionnaires.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text1))\n",
    "\n",
    "    code1 = f\"\"\"from sdv_enterprise.sdv.multi_table.dayz.day_zero import \\\n",
    "DayZSynthesizer\n",
    "    \\nsynthesizer = DayZSynthesizer(metadata)\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code1))\n",
    "\n",
    "    code0 = \"\"\"from mod import map_category_values\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code0))\n",
    "\n",
    "    code = f\"\"\"dico = {dico_list}\n",
    "\n",
    "map_category_values(synthesizer, dico)\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code))\n",
    "\n",
    "    code1_1 = \"\"\"# synthesizer.get_parameters()\n",
    "# decommentez la fonction au dessus pour voir tous les dictionnaires \\\n",
    "ajoute a votre synthetiseur\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code1_1))\n",
    "\n",
    "    text2 = \"\"\"# Generons vos donnees\n",
    "Lancez la commande suivante pour generer vos donnees. N'oubliez \\\n",
    "pas que vous pouvez changer le nombre de ligne pour toutes les tables en \\\n",
    "changeant le nombre en bas.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text2))\n",
    "\n",
    "    code2 = f\"\"\"synthetic_data = synthesizer.sample(num_rows = 1000)\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code2))\n",
    "\n",
    "    code2_1 = \"\"\"synthetic_data['nom_de_table'] # Petite visualisation\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code2_1))\n",
    "\n",
    "    text3 = \"\"\"# Enregistrer les donnees synthetiques\n",
    "Lancez la fonction suivante afin d'obtenir un dossier zip avec toutes les \\\n",
    "tables a l'interieur. Le dossier sera sous le nom de **synthetic_data.zip**\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text3))\n",
    "\n",
    "    code3 = f\"\"\"import zipfile\n",
    "from io import BytesIO\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "    for filename, df in synthetic_data.items():\n",
    "        csv_buffer = BytesIO()\n",
    "        df.to_csv(csv_buffer, index=False, sep = '{sep}')\n",
    "        csv_buffer.seek(0)\n",
    "        \n",
    "        filename_csv = filename + \".csv\"\n",
    "        zip_file.writestr(filename_csv, csv_buffer.getvalue())\n",
    "\n",
    "with open('synthetic_data.zip', 'wb') as f:\n",
    "    f.write(zip_buffer.getvalue())\n",
    "\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code3))\n",
    "    \n",
    "    notebook_filename = \"1-DayZ_Generation.ipynb\"\n",
    "    with open(notebook_filename, 'w') as f:\n",
    "        nbf.write(nb, f)\n",
    "\n",
    "\n",
    "def create_with_data(output, data_path, sep):\n",
    "    \"\"\"Creates the second notebook if the scratch setting was set to FALSE\n",
    "    for the model to learn from real data and sets the seperator of the \n",
    "    exported CSV. If the separator was precised then it will be applied \n",
    "    otherwise the default separator is \",\"\n",
    "    \n",
    "    Args:\n",
    "        output (str): The name of the created file and we will add _v2 in order\n",
    "        to follow the first notebook\n",
    "        data_path (list): The path to the data in order to train the model\n",
    "        sep (str): The default or custom separator for the exported CSV\n",
    "    \"\"\"    \n",
    "    nb = nbf.v4.new_notebook()\n",
    "    \n",
    "    text = \"\"\"# Generation de donnees synthetique a partir du Metadata et de \\\n",
    "    donnees existantes (apprentissage)\n",
    "Si vous lancez les cases suivantes, il vous sera possible de generer des \\\n",
    "donnees synthetiques a partir de vos donnees reelles. Vous n'avez pas besoin \\\n",
    "d'importer les dictionnaires, les valeurs des colonnes categoriques seront \\\n",
    "detectees automatiquement. Il vous suffit juste de verifier si vous avez \\\n",
    "charge le bon metadata et d'avoir charge les bonnes donnees et enfin, de \\\n",
    "lancer toutes les cases. \n",
    "\n",
    "***Attention! Le liens entre les tables sont faites de maniere automatique \\\n",
    "par le convertisseur. Si vos tables possedent deja une logique et des \\\n",
    "liens, vous avez encore la possibilite de modifier les relations \\\n",
    "entre les tables pour que l'outil fonctionne correctement. Faites les \\\n",
    "modifications avant l'import du modele. Sinon vous n'arriverez pas a \\\n",
    "synthetiser les donnees a partir du modele d'apprentissage et il faudra \\\n",
    "passer par la generation \"from scratch\"*** \n",
    "\n",
    "## Visualisation\n",
    "Verifiez si votre Metadata est correct\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text))\n",
    "\n",
    "    code = f\"\"\"from sdv.metadata import MultiTableMetadata\n",
    "    \\nimport pandas as pd\n",
    "    \\nmetadata = MultiTableMetadata.load_from_json('./{output}_v2.json')\n",
    "    \\nmetadata.visualize()\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code))\n",
    "\n",
    "    text2 = \"\"\"# Import de vos donnees\n",
    "Lancez la case suivante afin de charger vos donnees reelles pour \\\n",
    "l'apprentissage du modele. Il faut noter que le modele va repliquer toutes \\\n",
    "les subtilites statistiques de vos donnees (repartitions, distributions, \\\n",
    "correlations...) mais ne va pas attribuer de logique reelles. Ainsi, il \\\n",
    "n'oubliez pas que vous avez la possibilite d'ajouter des \\\n",
    "[contraintes](https://epiconcept.gitbook.io/synthetic-data-vault-sdv/multi-tables/modelisation/personnalisations) \\\n",
    "pour rendre vos donnees plus realistes. La documentation pandas peut vous \\\n",
    "servir pour l'import des tables de donnees CSV en cas de choses differentes. \\\n",
    "La variable 'datasets' joue un role de dictionnaires dans lequel vous pouvez \\\n",
    "chercher vos tables avec le nom de la table en cle. Si vos CSV suivent un \\\n",
    "format specifique ou possedent des caracteristiques particulieres vous pouvez \\\n",
    "allez voir les \\\n",
    "[parametres d'import](https://epiconcept.gitbook.io/synthetic-data-vault-sdv/multi-tables/preparation-des-donnees/charger-les-donnees#load_csv).\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text2))\n",
    "\n",
    "    path_to_data = './data/' + data_path\n",
    "\n",
    "    code2 = f\"\"\"from sdv.datasets.local import load_csvs\n",
    "\\ndatasets = load_csvs(\n",
    "    folder_name = '{path_to_data}')\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code2))\n",
    "\n",
    "    code3 = \"\"\"datasets['nom_de_table'].head() # Visualisez \\\n",
    "    l'une de vos tables grace a cette commande. Il vous suffit d'ajouter le \\\n",
    "nom d'une des tables dans vos jeu de donnees.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code3))\n",
    "\n",
    "    text3 = \"\"\"# Import du modele et apprentissage\n",
    "Vous allez ici faire apprendre au modele les caracteristiques statistiques \\\n",
    "de vos donnees reelles. Lancez les codes suivants afin de faire apprendre \\\n",
    "votre modele. Nous avons ici le modele le plus performant qui va travailler \\\n",
    "sur les relations intra-table mais aussi inter-tables. Le modele est le \\\n",
    "[HSASynthesizer](https://epiconcept.gitbook.io/synthetic-data-vault-sdv/multi-tables/modelisation/synthetiseurs/hsa-synthesizer) \\\n",
    "vous pouvez essayer d'autres synthetiseur \\\n",
    "[ici](https://epiconcept.gitbook.io/synthetic-data-vault-sdv/multi-tables/modelisation/synthetiseurs)\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text3))\n",
    "\n",
    "    code4 = \"\"\"from sdv_enterprise.sdv.multi_table.hsa import HSASynthesizer\n",
    "synthesizer = HSASynthesizer(metadata) # Nous precisons la structures \\\n",
    "des donnees.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code4))\n",
    "\n",
    "    code5 = \"\"\"synthesizer.fit(datasets) # L'apprentissage se fait a cette \\\n",
    "    etape. Le temps d'apprentissage peut etre different selon la taille de \\\n",
    "        vos donnees.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code5))\n",
    "\n",
    "    text4 = \"\"\"# Generation des donnees synthetiques\n",
    "Si vous avez deja genere des donnees synthetiques, le principe reste le meme. \\\n",
    "Ici nous allons defenir comme au dessus une sorte de bibliotheque qui va \\\n",
    "contenir toutes nos donnees et on va pouvoir generer cette fois une echelle. \\\n",
    "L'echelle est le nombre de fois que le modele va multiplier la quantite de \\\n",
    "donnee initiale. Si vous voulez 2 fois la quantite de base de vos donnees \\\n",
    "il vous suffit de mettre '2'.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text4))\n",
    "\n",
    "    code6 = \"\"\"synthetic_data = synthesizer.sample(scale=1) # Ici nous \\\n",
    "    voulons que la quantite generee soit identique aux donnees reelles.\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code6))\n",
    "\n",
    "    code6_1 = \"\"\"synthetic_data['nom_de_table'] # Petite visualisation\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code6_1))\n",
    "\n",
    "    text5 = \"\"\"# Enregistrer les donnees synthetiques\n",
    "Lancez la fonction suivante afin d'obtenir un dossier zip avec toutes les \\\n",
    "tables a l'interieur. Le dossier sera sous le nom de **synthetic_data.zip**\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(text5))\n",
    "\n",
    "    code7 = f\"\"\"import zipfile\n",
    "from io import BytesIO\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "    for filename, df in synthetic_data.items():\n",
    "        csv_buffer = BytesIO()\n",
    "        df.to_csv(csv_buffer, index=False, sep = '{sep}')\n",
    "        csv_buffer.seek(0)\n",
    "        \n",
    "        filename_csv = filename + \".csv\"\n",
    "        zip_file.writestr(filename_csv, csv_buffer.getvalue())\n",
    "\n",
    "with open('synthetic_data.zip', 'wb') as f:\n",
    "    f.write(zip_buffer.getvalue())\n",
    "\"\"\"\n",
    "    nb.cells.append(nbf.v4.new_code_cell(code7))\n",
    "\n",
    "\n",
    "    notebook_filename = \"1-Learning_Generation.ipynb\"\n",
    "    with open(notebook_filename, 'w') as f:\n",
    "        nbf.write(nb, f)\n",
    "\n",
    "\n",
    "def main(input_file):\n",
    "    \"\"\"Takes the YAML config file to set up everything.\n",
    "\n",
    "    Args:\n",
    "        input_file (.yml): YML file that contains all the set up.\n",
    "            input file : either json_file_path or folder_path\n",
    "            output_file : name of the metadata.json file\n",
    "            new_folder : the new folder that will contains all the files\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        output_file = config['output_metadata_name']\n",
    "        folder_name = config['generation_folder_name']\n",
    "        scratch = config['scratch_mode']\n",
    "\n",
    "    if 'separator' in config:\n",
    "        separator = config['separator']\n",
    "    else:\n",
    "        separator = ','\n",
    "    if 'calculated' in config:\n",
    "        calc = config['calculated']\n",
    "        print(f\"Your calculated values are taken in account. You precised : \\\n",
    "{calc}.\")\n",
    "    else:\n",
    "        print(\"All your calculated values are by default not taken account.\",\n",
    "              \"If otherwise please specify it in the configuration file.\")\n",
    "        calc = False\n",
    "\n",
    "    if 'input_folder_path' in config:\n",
    "        f_p = './data/' + config['input_folder_path']\n",
    "        metadata_json, dico_list = import_csv_folder(f_p)\n",
    "        create_folder(folder_name)\n",
    "        create_notebook(folder_name, output_file, metadata_json)\n",
    "        if scratch == True:\n",
    "            create_from_scratch(output_file, dico_list,separator)\n",
    "            print('DayZSynthesizer notebook created.')\n",
    "        elif scratch == False:\n",
    "            data_folder_name = config['data_folder_name']\n",
    "            if 'data_folder_name' in config:\n",
    "                create_with_data(output_file, data_folder_name, separator)\n",
    "                print('HSASynthesizer notebook created.')\n",
    "            else:\n",
    "                print('Please input your data folder name.')\n",
    "        else:\n",
    "            print(\"Input Scratch answer. Or you won't get any data \\\n",
    "generetion notebook.\")\n",
    "    elif 'input_json_file_path' in config:\n",
    "        j_p = './data/' + config['input_json_file_path']\n",
    "        metadata_json,  dico_list = import_json_file(j_p, calc)\n",
    "        create_folder(folder_name)\n",
    "        create_notebook(folder_name, output_file, metadata_json)\n",
    "        if scratch == True:\n",
    "            create_from_scratch(output_file, dico_list, separator)\n",
    "            print('DayZSynthesizer notebook created.')\n",
    "        elif scratch == False:\n",
    "            data_folder_name = config['data_folder_name']\n",
    "            if 'data_folder_name' in config:\n",
    "                create_with_data(output_file, data_folder_name, separator)\n",
    "                print('HSASynthesizer notebook created.')\n",
    "            else:\n",
    "                print('Please input your data folder name.')\n",
    "        else:\n",
    "            print(\"Input Scratch answer. Or you won't get any data \\\n",
    "generetion notebook.\")\n",
    "    else:\n",
    "        print(\"Invalid input file format.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python app.py /app/data/<input_yaml_file>\")\n",
    "    else:\n",
    "        input_yaml_file = sys.argv[1]\n",
    "        main(input_yaml_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
